{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thanh-abaii/OpenDeepResearcher/blob/main/open_deep_researcher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio\n",
        "!pip install tavily-python\n",
        "!pip install groq\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7cTpP9rDZW-",
        "outputId": "3a4a3f1e-0dbe-4051-bcfd-09a8b8e7a9eb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Collecting tavily-python\n",
            "  Downloading tavily_python-0.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from tavily-python) (2.32.3)\n",
            "Collecting tiktoken>=0.5.1 (from tavily-python)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from tavily-python) (0.28.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken>=0.5.1->tavily-python) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->tavily-python) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->tavily-python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->tavily-python) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->tavily-python) (2024.12.14)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->tavily-python) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->tavily-python) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->tavily-python) (0.14.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->tavily-python) (1.3.1)\n",
            "Downloading tavily_python-0.5.0-py3-none-any.whl (14 kB)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, tavily-python\n",
            "Successfully installed tavily-python-0.5.0 tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import aiohttp\n",
        "import os\n",
        "from tavily import TavilyClient\n",
        "from google.colab import userdata\n",
        "\n",
        "# =======================\n",
        "# Configuration Constants\n",
        "# =======================\n",
        "OPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')  # Replace with your OpenRouter API key\n",
        "TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')  # Replace with your Tavily API key\n",
        "\n",
        "OPENROUTER_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "DEFAULT_MODEL = \"deepseek/deepseek-r1-distill-qwen-32b\"\n",
        "\n",
        "# Instantiate Tavily Client (synchronous, used via asyncio.to_thread)\n",
        "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)"
      ],
      "metadata": {
        "id": "izdz1cRgTZfI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = tavily_client.search(\"Who is Leo Messi?\")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "KAwEyw0yWYCv",
        "outputId": "16bd5931-8890-457a-a255-741af7a5b5dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'Who is Leo Messi?', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'Career of Lionel Messi - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Career_of_Lionel_Messi', 'content': \"Lionel Messi is an Argentine professional footballer who plays as a forward for and captains both Major League Soccer club Inter Miami and the Argentina national team.His individual achievements include eight Ballon d'Or awards, the most for any footballer. Having won 45 team trophies, [note 1] he is the most decorated player in the history of professional football. [11]\", 'score': 0.8405867, 'raw_content': None}, {'title': 'Lionel Messi - Wikipedia', 'url': 'https://en.wikipedia.org/wiki/Lionel_Messi', 'content': 'He scored twice in the last group match, a 3–2 victory over Nigeria, his second goal coming from a free kick, as they finished first in their group.[423] Messi assisted a late goal in extra time to ensure a 1–0 win against Switzerland in the round of 16, and played in the 1–0 quarter-final win against Belgium as Argentina progressed to the semi-final of the World Cup for the first time since 1990.[424][425] Following a 0–0 draw in extra time, they eliminated the Netherlands 4–2 in a penalty shootout to reach the final, with Messi scoring his team\\'s first penalty.[426]\\nBilled as Messi versus Germany, the world\\'s best player against the best team, the final was a repeat of the 1990 final featuring Diego Maradona.[427] Within the first half-hour, Messi had started the play that led to a goal, but it was ruled offside. \"[582] Moreover, several pundits and footballing figures, including Maradona, questioned Messi\\'s leadership with Argentina at times, despite his playing ability.[583][584][585] Vickery states the perception of Messi among Argentines changed in 2019, with Messi making a conscious effort to become \"more one of the group, more Argentine\", with Vickery adding that following the World Cup victory in 2022 Messi would now be held in the same esteem by his compatriots as Maradona.[581]\\nComparisons with Cristiano Ronaldo\\nAmong his contemporary peers, Messi is most often compared and contrasted with Portuguese forward Cristiano Ronaldo, as part of an ongoing rivalry that has been compared to past sports rivalries like the Muhammad Ali–Joe Frazier rivalry in boxing, the Roger Federer–Rafael Nadal rivalry in tennis, and the Prost–Senna rivalry from Formula One motor racing.[586][587]\\nAlthough Messi has at times denied any rivalry,[588][589] they are widely believed to push one another in their aim to be the best player in the world.[160] Since 2008, Messi has won eight Ballons d\\'Or to Ronaldo\\'s five,[590] seven FIFA World\\'s Best Player awards to Ronaldo\\'s five, and six European Golden Shoes to Ronaldo\\'s four.[591] Pundits and fans regularly argue the individual merits of both players.[160][592] On 11 July, Messi provided his 20th assist of the league season for Arturo Vidal in a 1–0 away win over Real Valladolid, equalling Xavi\\'s record of 20 assists in a single La Liga season from 2008 to 2009;[281][282] with 22 goals, he also became only the second player ever, after Thierry Henry in the 2002–03 FA Premier League season with Arsenal (24 goals and 20 assists), to record at least 20 goals and 20 assists in a single league season in one of Europe\\'s top-five leagues.[282][283] Following his brace in a 5–0 away win against Alavés in the final match of the season on 20 May, Messi finished the season as both the top scorer and top assist provider in La Liga, with 25 goals and 21 assists respectively, which saw him win his record seventh Pichichi trophy, overtaking Zarra; however, Barcelona missed out on the league title to Real Madrid.[284] On 7 March, two weeks after scoring four goals in a league fixture against Valencia, he scored five times in a Champions League last 16-round match against Bayer Leverkusen, an unprecedented achievement in the history of the competition.[126][127] In addition to being the joint top assist provider with five assists, this feat made him top scorer with 14 goals, tying José Altafini\\'s record from the 1962–63 season, as well as becoming only the second player after Gerd Müller to be top scorer in four campaigns.[128][129] Two weeks later, on 20 March, Messi became the top goalscorer in Barcelona\\'s history at 24 years old, overtaking the 57-year record of César Rodríguez\\'s 232 goals with a hat-trick against Granada.[130]\\nDespite Messi\\'s individual form, Barcelona\\'s four-year cycle of success under Guardiola – one of the greatest eras in the club\\'s history – drew to an end.[131] He still managed to break two longstanding records in a span of seven days: a hat-trick on 16 March against Osasuna saw him overtake Paulino Alcántara\\'s 369 goals to become Barcelona\\'s top goalscorer in all competitions including friendlies, while another hat-trick against Real Madrid on 23 March made him the all-time top scorer in El Clásico, ahead of the 18 goals scored by former Real Madrid player Alfredo Di Stéfano.[160][162] Messi finished the campaign with his worst output in five seasons, though he still managed to score 41 goals in all competitions.[161][163] For the first time in five years, Barcelona ended the season without a major trophy; they were defeated in the Copa del Rey final by Real Madrid and lost the league in the last game to Atlético Madrid, causing Messi to be booed by sections of fans at the Camp Nou.[164]', 'score': 0.6396691, 'raw_content': None}, {'title': 'Lionel Messi - Simple English Wikipedia, the free encyclopedia', 'url': 'https://simple.wikipedia.org/wiki/Lionel_Messi', 'content': 'He told the court he \"only played football\" and didn\\'t know anything because he left his money problems to be dealt with by his father, Jorge Messi.[23]\\nClub career statistics[change | change source]\\nInternational career statistics[change | change source]\\nHonours[change | change source]\\nClub[change | change source]\\nBarcelona[26]\\nInternational[change | change source]\\nArgentina U20\\nArgentina Olympic team\\nArgentina Senior team\\nIndividual[change | change source]\\nNotes[change | change source]\\nReferences[change | change source] He scored his first hat-trick with the country in his 68th appearance during a 3-1 victory against Switzerland in February 2012.[17]\\nOn 21 June 2016, he broke the top scoring record for Argentina by scoring his 55th goal with a gem of a free-kick against the United States in the Copa América Centenario. In an emotional interview after the game, he said he was very sad that he missed the penalty and also sad that he wasn\\'t able to win any trophies in four finals.[18][19][20]\\nHe came out of retirement in October 2016 for the 2018 World Cup qualifiers.\\n On 18 April 2007, he scored a goal almost exactly identical to Maradona\\'s \"Goal of the Century\" against England in the 1986 World Cup, where Maradona got the ball behind the halfway line on the right side and beat 4 defenders and the goalie to score. Barcelona won the La Liga that season for the first time in 6 years, and won the league for a second time in a row along with the Spanish Super Cup and UEFA Champions League in 2006.\\n', 'score': 0.60386235, 'raw_content': None}, {'title': 'Lionel Messi Biography - Facts, Childhood, Family Life & Achievements', 'url': 'https://www.thefamouspeople.com/profiles/lionel-messi-5242.php', 'content': \"Lionel Messi has won multiple FIFA Ballon d'Or awards, numerous La Liga titles with FC Barcelona, and holds the record for most goals scored in a calendar year. In the finals too Messi scored the winning goal to give Barcelona their third title in six years and fourth overall. In the 2018 Football World Cup Messi scored a goal in the Argentina's final group match against Nigeria and helped his team to a 2-1 victory. Messi has in his kitty 20 Player of the Year awards including FIFA World Player of the Year (1), World Soccer Player of the Year (3), Goal.com Player of the Year (2), UEFA Best Player in Europe Award (1), UEFA Club Footballer of the Year (1), FIFA U-20 World Cup Player of the Tournament (1), La Liga Player of the Year (3), La Liga Foreign Player of the Year (3) and La Liga Ibero-American Player of the Year (5).\", 'score': 0.57952, 'raw_content': None}, {'title': \"Lionel Messi | Biography, Trophies, Records, Ballon d'Or, Inter Miami ...\", 'url': 'https://www.britannica.com/biography/Lionel-Messi', 'content': 'In early 2009 Messi capped off a spectacular 2008–09 season by helping FC Barcelona capture the club’s first “treble” (winning three major European club titles in one season): the team won the La Liga championship, the Copa del Rey (Spain’s major domestic cup), and the Champions League title. Messi’s play continued to rapidly improve over the years, and by 2008 he was one of the most dominant players in the world, finishing second to Manchester United’s Cristiano Ronaldo in the voting for the 2008 Ballon d’Or. At the 2014 World Cup, Messi put on a dazzling display, scoring four goals and almost single-handedly propelling an offense-deficient Argentina team through the group stage and into the knockout rounds, where Argentina then advanced to the World Cup final for the first time in 24 years. After Argentina was defeated in the Copa final—the team’s third consecutive finals loss in a major tournament—Messi said that he was quitting the national team, but his short-lived “retirement” lasted less than two months before he announced his return to the Argentine team. Messi helped Barcelona capture another treble during the 2014–15 season, leading the team with 43 goals scored over the course of the campaign, which resulted in his fifth world player of the year honour.', 'score': 0.54971373, 'raw_content': None}], 'response_time': 1.57}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import aiohttp\n",
        "from tavily import TavilyClient\n",
        "from google.colab import userdata\n",
        "\n",
        "# =======================\n",
        "# Configuration Constants\n",
        "# =======================\n",
        "OPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')  # Replace with your OpenRouter API key\n",
        "TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')            # Replace with your Tavily API key\n",
        "\n",
        "OPENROUTER_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "DEFAULT_MODEL = \"deepseek/deepseek-r1-distill-qwen-32b\"\n",
        "\n",
        "# Instantiate Tavily Client (synchronous, used via asyncio.to_thread)\n",
        "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)\n",
        "\n",
        "# ============================\n",
        "# Asynchronous Helper Functions\n",
        "# ============================\n",
        "\n",
        "async def call_openrouter_async(session, messages, model=DEFAULT_MODEL):\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"X-Title\": \"OpenDeepResearcher, by Matt Shumer\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": messages\n",
        "    }\n",
        "    try:\n",
        "        async with session.post(OPENROUTER_URL, headers=headers, json=payload) as resp:\n",
        "            if resp.status == 200:\n",
        "                result = await resp.json()\n",
        "                try:\n",
        "                    return result['choices'][0]['message']['content']\n",
        "                except (KeyError, IndexError):\n",
        "                    print(\"Unexpected OpenRouter response structure:\", result)\n",
        "                    return None\n",
        "            else:\n",
        "                text = await resp.text()\n",
        "                print(f\"OpenRouter API error: {resp.status} - {text}\")\n",
        "                return None\n",
        "    except Exception as e:\n",
        "        print(\"Error calling OpenRouter:\", e)\n",
        "        return None\n",
        "\n",
        "async def generate_search_queries_async(session, user_query):\n",
        "    prompt = (\n",
        "        \"You are an expert research assistant. Given the user's query, generate up to four distinct, \"\n",
        "        \"precise search queries that would help gather comprehensive information on the topic. \"\n",
        "        \"Return only a Python list of strings, for example: ['query1', 'query2', 'query3'].\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful and precise research assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    response = await call_openrouter_async(session, messages)\n",
        "    if response:\n",
        "        try:\n",
        "            search_queries = eval(response)\n",
        "            if isinstance(search_queries, list):\n",
        "                return search_queries\n",
        "            else:\n",
        "                print(\"LLM did not return a list. Response:\", response)\n",
        "                return []\n",
        "        except Exception as e:\n",
        "            print(\"Error parsing search queries:\", e, \"\\nResponse:\", response)\n",
        "            return []\n",
        "    return []\n",
        "\n",
        "async def perform_search_async(session, query):\n",
        "    \"\"\"\n",
        "    Asynchronously perform a search using Tavily Search for the given query.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = await asyncio.to_thread(tavily_client.search, query)\n",
        "        links = []\n",
        "        # Tavily API might return results in a key \"results\" (or \"organic_results\")\n",
        "        if \"organic_results\" in response:\n",
        "            links = [item.get(\"link\") for item in response[\"organic_results\"] if \"link\" in item]\n",
        "        elif \"results\" in response:\n",
        "            links = [item.get(\"url\") for item in response[\"results\"] if \"url\" in item]\n",
        "        else:\n",
        "            print(\"No recognizable results in Tavily search response for query:\", query)\n",
        "        return links\n",
        "    except Exception as e:\n",
        "        print(\"Error performing Tavily search:\", e)\n",
        "        return []\n",
        "\n",
        "async def extract_webpage_text_async(session, url):\n",
        "    \"\"\"\n",
        "    Asynchronously extract the raw text content of a webpage using Tavily's extraction API.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Tavily's extract expects a list of URLs.\n",
        "        response = await asyncio.to_thread(tavily_client.extract, urls=[url], include_images=False)\n",
        "        if \"results\" in response and len(response[\"results\"]) > 0:\n",
        "            result = response[\"results\"][0]\n",
        "            return result.get(\"raw_content\", \"\")\n",
        "        else:\n",
        "            print(f\"No extraction result for URL: {url}\")\n",
        "            return \"\"\n",
        "    except Exception as e:\n",
        "        print(\"Error extracting webpage text with Tavily:\", e)\n",
        "        return \"\"\n",
        "\n",
        "async def is_page_useful_async(session, user_query, page_text):\n",
        "    prompt = (\n",
        "        \"You are a critical research evaluator. Given the user's query and the content of a webpage, \"\n",
        "        \"determine if the webpage contains information relevant and useful for addressing the query. \"\n",
        "        \"Respond with exactly one word: 'Yes' if the page is useful, or 'No' if it is not. Do not include any extra text.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a strict and concise evaluator of research relevance.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\n\\nWebpage Content (first 20000 characters):\\n{page_text[:20000]}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    response = await call_openrouter_async(session, messages)\n",
        "    if response:\n",
        "        answer = response.strip()\n",
        "        if answer in [\"Yes\", \"No\"]:\n",
        "            return answer\n",
        "        else:\n",
        "            if \"Yes\" in answer:\n",
        "                return \"Yes\"\n",
        "            elif \"No\" in answer:\n",
        "                return \"No\"\n",
        "    return \"No\"\n",
        "\n",
        "async def extract_relevant_context_async(session, user_query, search_query, page_text):\n",
        "    prompt = (\n",
        "        \"You are an expert information extractor. Given the user's query, the search query that led to this page, \"\n",
        "        \"and the webpage content, extract all pieces of information that are relevant to answering the user's query. \"\n",
        "        \"Return only the relevant context as plain text without commentary.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert in extracting and summarizing relevant information.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\nSearch Query: {search_query}\\n\\nWebpage Content (first 20000 characters):\\n{page_text[:20000]}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    response = await call_openrouter_async(session, messages)\n",
        "    if response:\n",
        "        return response.strip()\n",
        "    return \"\"\n",
        "\n",
        "async def get_new_search_queries_async(session, user_query, previous_search_queries, all_contexts):\n",
        "    context_combined = \"\\n\".join(all_contexts) if all_contexts else \"\"\n",
        "    prompt = (\n",
        "        \"You are an analytical research assistant. Based on the original query, the search queries performed so far, \"\n",
        "        \"and the extracted contexts from webpages (if any), determine if further research is needed. \"\n",
        "        \"If further research is needed, provide up to four new search queries as a Python list (for example, \"\n",
        "        \"['new query1', 'new query2']). If you believe no further research is needed, respond with exactly <done>.\"\n",
        "        \"\\nOutput only a Python list or the token <done> without any additional text.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a systematic research planner.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\nPrevious Search Queries: {previous_search_queries}\\n\\nExtracted Relevant Contexts:\\n{context_combined}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    response = await call_openrouter_async(session, messages)\n",
        "    if response:\n",
        "        cleaned = response.strip()\n",
        "        if cleaned == \"<done>\":\n",
        "            return \"<done>\"\n",
        "        try:\n",
        "            new_queries = eval(cleaned)\n",
        "            if isinstance(new_queries, list):\n",
        "                return new_queries\n",
        "            else:\n",
        "                print(\"LLM did not return a list for new search queries. Response:\", response)\n",
        "                return []\n",
        "        except Exception as e:\n",
        "            print(\"Error parsing new search queries:\", e, \"\\nResponse:\", response)\n",
        "            return []\n",
        "    return []\n",
        "\n",
        "async def generate_final_report_async(session, user_query, all_contexts):\n",
        "    if not all_contexts:\n",
        "        return f\"No relevant content was found for the query: {user_query}.\"\n",
        "    context_combined = \"\\n\".join(all_contexts)\n",
        "    prompt = (\n",
        "        \"You are an expert researcher and report writer. Based on the gathered contexts below and the original query, \"\n",
        "        \"write a comprehensive, well-structured, and detailed report that addresses the query thoroughly. \"\n",
        "        \"Include all relevant insights and conclusions without extraneous commentary.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a skilled report writer.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\n\\nGathered Relevant Contexts:\\n{context_combined}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    report = await call_openrouter_async(session, messages)\n",
        "    return report\n",
        "\n",
        "async def process_link(session, link, user_query, search_query):\n",
        "    print(f\"Fetching and extracting content from: {link}\")\n",
        "    page_text = await extract_webpage_text_async(session, link)\n",
        "    if not page_text:\n",
        "        return None\n",
        "    usefulness = await is_page_useful_async(session, user_query, page_text)\n",
        "    print(f\"Page usefulness for {link}: {usefulness}\")\n",
        "    if usefulness == \"Yes\":\n",
        "        context = await extract_relevant_context_async(session, user_query, search_query, page_text)\n",
        "        if context:\n",
        "            print(f\"Extracted context from {link} (first 200 chars): {context[:200]}\")\n",
        "            return context\n",
        "    return None\n",
        "\n",
        "# =========================\n",
        "# Main Asynchronous Routine\n",
        "# =========================\n",
        "\n",
        "async def async_main():\n",
        "    user_query = input(\"Enter your research query/topic: \").strip()\n",
        "    iter_limit_input = input(\"Enter maximum number of iterations (default 10): \").strip()\n",
        "    iteration_limit = int(iter_limit_input) if iter_limit_input.isdigit() else 10\n",
        "\n",
        "    aggregated_contexts = []    # All useful contexts from every iteration\n",
        "    all_search_queries = []     # Every search query used across iterations\n",
        "    iteration = 0\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        # ----- INITIAL SEARCH QUERIES -----\n",
        "        new_search_queries = await generate_search_queries_async(session, user_query)\n",
        "        if not new_search_queries:\n",
        "            print(\"No search queries were generated by the LLM. Exiting.\")\n",
        "            return\n",
        "        all_search_queries.extend(new_search_queries)\n",
        "\n",
        "        # ----- ITERATIVE RESEARCH LOOP -----\n",
        "        while iteration < iteration_limit:\n",
        "            print(f\"\\n=== Iteration {iteration + 1} ===\")\n",
        "            iteration_contexts = []\n",
        "\n",
        "            # For each search query, perform Tavily searches concurrently.\n",
        "            search_tasks = [perform_search_async(session, query) for query in new_search_queries]\n",
        "            search_results = await asyncio.gather(*search_tasks)\n",
        "\n",
        "            # Aggregate all unique links from all search queries of this iteration.\n",
        "            unique_links = {}\n",
        "            for idx, links in enumerate(search_results):\n",
        "                query = new_search_queries[idx]\n",
        "                for link in links:\n",
        "                    if link and link not in unique_links:\n",
        "                        unique_links[link] = query\n",
        "\n",
        "            if unique_links:\n",
        "                print(f\"Aggregated {len(unique_links)} unique links from this iteration:\")\n",
        "                for url, query in unique_links.items():\n",
        "                    print(f\"  - URL: {url} (from query: {query})\")\n",
        "            else:\n",
        "                print(\"No links found for the current search queries.\")\n",
        "\n",
        "            # Process each link concurrently.\n",
        "            if unique_links:\n",
        "                link_tasks = [\n",
        "                    process_link(session, link, user_query, unique_links[link])\n",
        "                    for link in unique_links\n",
        "                ]\n",
        "                link_results = await asyncio.gather(*link_tasks)\n",
        "                for res in link_results:\n",
        "                    if res:\n",
        "                        iteration_contexts.append(res)\n",
        "            else:\n",
        "                print(\"Skipping link processing due to lack of results.\")\n",
        "\n",
        "            if iteration_contexts:\n",
        "                aggregated_contexts.extend(iteration_contexts)\n",
        "            else:\n",
        "                print(\"No useful contexts were found in this iteration.\")\n",
        "\n",
        "            # ----- ASK THE LLM IF MORE SEARCHES ARE NEEDED -----\n",
        "            new_search_queries = await get_new_search_queries_async(session, user_query, all_search_queries, aggregated_contexts)\n",
        "            if new_search_queries == \"<done>\":\n",
        "                print(\"LLM indicated that no further research is needed.\")\n",
        "                break\n",
        "            elif new_search_queries:\n",
        "                print(\"LLM provided new search queries:\", new_search_queries)\n",
        "                all_search_queries.extend(new_search_queries)\n",
        "            else:\n",
        "                print(\"LLM did not provide any new search queries. Ending the loop.\")\n",
        "                break\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "        # ----- FINAL REPORT -----\n",
        "        print(\"\\nGenerating final report...\")\n",
        "        final_report = await generate_final_report_async(session, user_query, aggregated_contexts)\n",
        "        print(\"\\n==== FINAL REPORT ====\\n\")\n",
        "        print(final_report)\n",
        "\n",
        "def main():\n",
        "    asyncio.run(async_main())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "iiewljj1Pdeu",
        "outputId": "5632d81d-8331-40a3-fbbb-94225c62cb78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your research query/topic: DeepSeek-R1\n",
            "Enter maximum number of iterations (default 10): 3\n",
            "Unexpected OpenRouter response structure: {'error': {'message': 'More credits are required to run this request. 117877 token capacity required, 101717 available. To increase, visit https://openrouter.ai/credits and upgrade to a paid account', 'code': 402}}\n",
            "No search queries were generated by the LLM. Exiting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phiên bản cải thiện"
      ],
      "metadata": {
        "id": "azSQSTtraIko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import aiohttp\n",
        "from tavily import TavilyClient\n",
        "from google.colab import userdata\n",
        "\n",
        "# =======================\n",
        "# Configuration Constants\n",
        "# =======================\n",
        "OPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')\n",
        "TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')\n",
        "\n",
        "OPENROUTER_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "DEFAULT_MODEL = \"deepseek/deepseek-r1-distill-qwen-32b\"\n",
        "\n",
        "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)\n",
        "\n",
        "# ============================\n",
        "# Asynchronous Helper Functions\n",
        "# ============================\n",
        "\n",
        "async def call_openrouter_async(session, messages, model=DEFAULT_MODEL):\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "        \"X-Title\": \"OpenDeepResearcher, by Matt Shumer\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    payload = {\"model\": model, \"messages\": messages}\n",
        "\n",
        "    try:\n",
        "        async with session.post(OPENROUTER_URL, headers=headers, json=payload) as resp:\n",
        "            if resp.status == 200:\n",
        "                result = await resp.json()\n",
        "                return result.get('choices', [{}])[0].get('message', {}).get('content', None)\n",
        "            else:\n",
        "                print(f\"OpenRouter API error: {resp.status}\")\n",
        "                return None\n",
        "    except Exception as e:\n",
        "        print(\"Error calling OpenRouter:\", e)\n",
        "        return None\n",
        "\n",
        "async def generate_search_queries_async(session, user_query):\n",
        "    \"\"\"\n",
        "    Generate new search queries based on the user's research topic.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"You are a research assistant. Generate up to four precise search queries \"\n",
        "        \"that provide comprehensive information on the topic. Return a Python list, e.g.: ['query1', 'query2'].\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a precise research assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    response = await call_openrouter_async(session, messages)\n",
        "\n",
        "    if response:\n",
        "        # Loại bỏ dấu ```python\n",
        "        cleaned_response = response.replace(\"```python\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "        # Kiểm tra nếu phản hồi có dạng danh sách hợp lệ trước khi eval()\n",
        "        if cleaned_response.startswith(\"[\") and cleaned_response.endswith(\"]\"):\n",
        "            try:\n",
        "                search_queries = eval(cleaned_response)\n",
        "                if isinstance(search_queries, list):\n",
        "                    return search_queries\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing search queries: {e}\\nResponse: {cleaned_response}\")\n",
        "\n",
        "        print(f\"Invalid search query format: {cleaned_response}\")\n",
        "\n",
        "    return []\n",
        "\n",
        "\n",
        "async def perform_search_async(session, query):\n",
        "    \"\"\"\n",
        "    Perform a Tavily search with optimized parameters.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = await asyncio.to_thread(\n",
        "            tavily_client.search,\n",
        "            query=query,\n",
        "            max_results=10,  # Tăng số lượng kết quả để có nhiều dữ liệu hơn\n",
        "            include_raw_content=True,  # Lấy nội dung văn bản thô từ trang web\n",
        "            search_depth=\"advanced\"  # Tìm kiếm chuyên sâu hơn\n",
        "        )\n",
        "\n",
        "        return [item.get(\"url\") for item in response.get(\"results\", []) if \"url\" in item]\n",
        "    except Exception as e:\n",
        "        print(\"Error performing Tavily search:\", e)\n",
        "        return []\n",
        "\n",
        "\n",
        "async def extract_webpage_text_async(session, url):\n",
        "    \"\"\"\n",
        "    Asynchronously extract the raw text content of a webpage using Tavily's extraction API.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = await asyncio.to_thread(tavily_client.extract, urls=[url], include_images=False)\n",
        "\n",
        "        if \"results\" in response and response[\"results\"]:\n",
        "            return response[\"results\"][0].get(\"raw_content\", \"\").strip()\n",
        "\n",
        "        print(f\"⚠️ No extraction result for URL: {url} - Response: {response}\")\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error extracting webpage text with Tavily for {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "\n",
        "async def is_page_useful_async(session, user_query, page_text):\n",
        "    prompt = \"Is the webpage useful for answering the query? Respond with exactly 'Yes' or 'No'.\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a research evaluator.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\nWebpage Content:\\n{page_text[:20000]}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    response = await call_openrouter_async(session, messages)\n",
        "    return response.strip() if response in [\"Yes\", \"No\"] else \"No\"\n",
        "\n",
        "async def extract_relevant_context_async(session, user_query, search_query, page_text):\n",
        "    prompt = \"Extract only relevant information from the webpage content for answering the user's query.\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert information extractor.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\nSearch Query: {search_query}\\nWebpage Content:\\n{page_text[:20000]}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    response = await call_openrouter_async(session, messages)\n",
        "    return response.strip() if response else \"\"\n",
        "\n",
        "async def process_link(session, link, user_query, search_query):\n",
        "    print(f\"🔍 Fetching and extracting content from: {link}\")\n",
        "\n",
        "    # Thử trích xuất 2 lần nếu lần đầu thất bại\n",
        "    for attempt in range(2):\n",
        "        page_text = await extract_webpage_text_async(session, link)\n",
        "        if page_text:\n",
        "            break\n",
        "        print(f\"⚠️ Retry extracting content from {link} (Attempt {attempt + 1}/2)\")\n",
        "\n",
        "    if not page_text:\n",
        "        print(f\"❌ Failed to extract content from {link} after 2 attempts.\")\n",
        "        return None\n",
        "\n",
        "    usefulness = await is_page_useful_async(session, user_query, page_text)\n",
        "    if usefulness == \"Yes\":\n",
        "        return await extract_relevant_context_async(session, user_query, search_query, page_text)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "async def generate_final_report_async(session, user_query, all_contexts):\n",
        "    if not all_contexts:\n",
        "        return f\"No relevant content was found for: {user_query}.\"\n",
        "\n",
        "    context_combined = \"\\n\".join(all_contexts)\n",
        "    prompt = \"Write a detailed and well-structured report based on the gathered contexts and the original query.\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a skilled report writer.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\nGathered Contexts:\\n{context_combined}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    return await call_openrouter_async(session, messages)\n",
        "\n",
        "# =========================\n",
        "# Main Asynchronous Routine\n",
        "# =========================\n",
        "\n",
        "async def async_main():\n",
        "    user_query = input(\"Enter your research query/topic: \").strip()\n",
        "    iter_limit_input = input(\"Enter max iterations (default 10): \").strip()\n",
        "    iteration_limit = int(iter_limit_input) if iter_limit_input.isdigit() else 10\n",
        "\n",
        "    aggregated_contexts = []\n",
        "    all_search_queries = []\n",
        "    iteration = 0\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        new_search_queries = await generate_search_queries_async(session, user_query)\n",
        "        if not new_search_queries:\n",
        "            print(\"❌ No search queries generated. Exiting.\")\n",
        "            return\n",
        "        all_search_queries.extend(new_search_queries)\n",
        "\n",
        "        while iteration < iteration_limit:\n",
        "            print(f\"\\n=== Iteration {iteration + 1} ===\")\n",
        "            search_tasks = [perform_search_async(session, query) for query in new_search_queries]\n",
        "            search_results = await asyncio.gather(*search_tasks)\n",
        "\n",
        "            unique_links = {}\n",
        "            for idx, links in enumerate(search_results):\n",
        "                query = new_search_queries[idx]\n",
        "                for link in links:\n",
        "                    if link and link not in unique_links:\n",
        "                        unique_links[link] = query\n",
        "\n",
        "            if not unique_links:\n",
        "                print(\"⚠️ No links found. Trying again in next iteration...\")\n",
        "                iteration += 1\n",
        "                continue\n",
        "\n",
        "            print(f\"✅ Aggregated {len(unique_links)} unique links from this iteration.\")\n",
        "            link_tasks = [process_link(session, link, user_query, unique_links[link]) for link in unique_links]\n",
        "            link_results = await asyncio.gather(*link_tasks)\n",
        "\n",
        "            # Chỉ lấy các context hợp lệ\n",
        "            iteration_contexts = [res for res in link_results if res]\n",
        "            aggregated_contexts.extend(iteration_contexts)\n",
        "\n",
        "            # Nếu không có kết quả hữu ích, đừng dừng luôn, thử tiếp\n",
        "            if not iteration_contexts:\n",
        "                print(\"⚠️ No useful content extracted, continuing to next iteration...\")\n",
        "                iteration += 1\n",
        "                continue\n",
        "\n",
        "            new_search_queries = await generate_search_queries_async(session, user_query)\n",
        "            if not new_search_queries:\n",
        "                print(\"🚫 No new queries. Ending loop.\")\n",
        "                break\n",
        "            all_search_queries.extend(new_search_queries)\n",
        "            iteration += 1\n",
        "\n",
        "        # Nếu không có dữ liệu, báo cáo điều đó thay vì \"None\"\n",
        "        print(\"\\n📝 Generating final report...\")\n",
        "        if not aggregated_contexts:\n",
        "            print(\"❌ No relevant content was extracted. Report cannot be generated.\")\n",
        "            return\n",
        "\n",
        "        final_report = await generate_final_report_async(session, user_query, aggregated_contexts)\n",
        "        print(\"\\n==== FINAL REPORT ====\\n\")\n",
        "        print(final_report)\n",
        "\n",
        "\n",
        "def main():\n",
        "    asyncio.run(async_main())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "GD5YTXENaM8h",
        "outputId": "aa2e327a-d8ae-4ebe-df22-8012a22b5ccb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your research query/topic: DeepSeek-R1\n",
            "Enter max iterations (default 10): 3\n",
            "❌ No search queries generated. Exiting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dùng Groq và llama-3.3-70b-versatile model"
      ],
      "metadata": {
        "id": "PkSZg4s0hhgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import asyncio\n",
        "import aiohttp\n",
        "from tavily import TavilyClient\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "\n",
        "# =======================\n",
        "# Configuration Constants\n",
        "# =======================\n",
        "GROQ_API_KEY = userdata.get(\"GROQ_API_KEY\")  # API key Groq\n",
        "TAVILY_API_KEY = userdata.get(\"TAVILY_API_KEY\")  # API key Tavily\n",
        "\n",
        "DEFAULT_MODEL = \"mixtral-8x7b-32768\"\n",
        "\n",
        "# Giới hạn số lượng trang được xử lý mỗi vòng lặp để tránh quá tải token\n",
        "MAX_LINKS_PER_ITERATION = 10\n",
        "MAX_TOKENS_INPUT = 4000  # Giới hạn tokens để tránh vượt quá quota\n",
        "\n",
        "# Khởi tạo API client\n",
        "groq_client = Groq(api_key=GROQ_API_KEY)\n",
        "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)\n",
        "\n",
        "# ============================\n",
        "# Asynchronous Helper Functions\n",
        "# ============================\n",
        "\n",
        "async def call_groq_async(messages, model=DEFAULT_MODEL):\n",
        "    \"\"\"\n",
        "    Gửi yêu cầu đến Groq API để xử lý hội thoại.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Giới hạn số lượng tokens bằng cách cắt nội dung của các tin nhắn đầu vào\n",
        "        for msg in messages:\n",
        "            if \"content\" in msg:\n",
        "                msg[\"content\"] = msg[\"content\"][:MAX_TOKENS_INPUT]\n",
        "\n",
        "        chat_completion = await asyncio.to_thread(\n",
        "            groq_client.chat.completions.create,\n",
        "            messages=messages,\n",
        "            model=model,\n",
        "            temperature=0.7,\n",
        "            max_completion_tokens=500  # Giới hạn số tokens trong phản hồi\n",
        "        )\n",
        "        return chat_completion.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Groq API Error: {e}\")\n",
        "        return None\n",
        "\n",
        "async def generate_search_queries_async(user_query):\n",
        "    \"\"\"\n",
        "    Tạo danh sách các truy vấn tìm kiếm dựa trên truy vấn của người dùng.\n",
        "    \"\"\"\n",
        "    prompt = (\n",
        "        \"Generate up to four precise search queries as a Python list, e.g., ['query1', 'query2', 'query3'].\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a research assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    response = await call_groq_async(messages)\n",
        "\n",
        "    if response:\n",
        "        try:\n",
        "            search_queries = eval(response)\n",
        "            if isinstance(search_queries, list):\n",
        "                return search_queries\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing search queries: {e}\\nResponse: {response}\")\n",
        "\n",
        "    print(f\"⚠️ Invalid search query format: {response}\")\n",
        "    return []\n",
        "\n",
        "async def perform_search_async(query):\n",
        "    \"\"\"\n",
        "    Thực hiện tìm kiếm bằng Tavily API.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = await asyncio.to_thread(\n",
        "            tavily_client.search,\n",
        "            query=query,\n",
        "            max_results=5,  # Giảm số lượng kết quả để tránh quá tải\n",
        "            include_raw_content=True\n",
        "        )\n",
        "        return [item.get(\"url\") for item in response.get(\"results\", []) if \"url\" in item]\n",
        "    except Exception as e:\n",
        "        print(\"❌ Error performing Tavily search:\", e)\n",
        "        return []\n",
        "\n",
        "async def extract_webpage_text_async(url):\n",
        "    \"\"\"\n",
        "    Trích xuất nội dung văn bản từ một trang web bằng Tavily API.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = await asyncio.to_thread(tavily_client.extract, urls=[url], include_images=False)\n",
        "        if \"results\" in response and response[\"results\"]:\n",
        "            return response[\"results\"][0].get(\"raw_content\", \"\")[:MAX_TOKENS_INPUT]  # Cắt nội dung\n",
        "        print(f\"⚠️ No extraction result for URL: {url}\")\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error extracting webpage text with Tavily for {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "async def is_page_useful_async(user_query, page_text):\n",
        "    \"\"\"\n",
        "    Xác định xem nội dung của một trang có liên quan đến truy vấn người dùng hay không.\n",
        "    \"\"\"\n",
        "    prompt = \"Is this webpage useful for answering the query? Respond 'Yes' or 'No'.\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an evaluator.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\nWebpage Content:\\n{page_text[:MAX_TOKENS_INPUT]}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    response = await call_groq_async(messages)\n",
        "    return response.strip() if response in [\"Yes\", \"No\"] else \"No\"\n",
        "\n",
        "async def extract_relevant_context_async(user_query, search_query, page_text):\n",
        "    \"\"\"\n",
        "    Trích xuất nội dung liên quan từ trang web.\n",
        "    \"\"\"\n",
        "    prompt = \"Extract only relevant information from the webpage content.\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert extractor.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\nSearch Query: {search_query}\\nWebpage Content:\\n{page_text[:MAX_TOKENS_INPUT]}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    response = await call_groq_async(messages)\n",
        "    return response.strip() if response else \"\"\n",
        "\n",
        "async def generate_final_report_async(user_query, all_contexts):\n",
        "    \"\"\"\n",
        "    Tạo báo cáo cuối cùng từ tất cả các nội dung đã thu thập.\n",
        "    \"\"\"\n",
        "    if not all_contexts:\n",
        "        return f\"No relevant content found for: {user_query}.\"\n",
        "\n",
        "    context_combined = \"\\n\".join(all_contexts)\n",
        "    prompt = \"Write a detailed report based on the gathered contexts and the query.\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a report writer.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\nGathered Contexts:\\n{context_combined[:MAX_TOKENS_INPUT]}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    return await call_groq_async(messages)\n",
        "\n",
        "async def process_link(link, user_query, search_query):\n",
        "    \"\"\"\n",
        "    Xử lý từng link: trích xuất nội dung, đánh giá và lọc thông tin.\n",
        "    \"\"\"\n",
        "    print(f\"🔍 Fetching and extracting content from: {link}\")\n",
        "    page_text = await extract_webpage_text_async(link)\n",
        "    if not page_text:\n",
        "        return None\n",
        "\n",
        "    usefulness = await is_page_useful_async(user_query, page_text)\n",
        "    if usefulness == \"Yes\":\n",
        "        return await extract_relevant_context_async(user_query, search_query, page_text)\n",
        "    return None\n",
        "\n",
        "# =========================\n",
        "# Main Asynchronous Routine\n",
        "# =========================\n",
        "\n",
        "async def async_main():\n",
        "    user_query = input(\"Enter your research query/topic: \").strip()\n",
        "    iteration_limit = 2  # Giảm số vòng lặp để tránh quá tải token\n",
        "\n",
        "    aggregated_contexts = []\n",
        "    all_search_queries = await generate_search_queries_async(user_query)\n",
        "    if not all_search_queries:\n",
        "        print(\"❌ No search queries generated. Exiting.\")\n",
        "        return\n",
        "\n",
        "    for iteration in range(iteration_limit):\n",
        "        print(f\"\\n=== Iteration {iteration + 1} ===\")\n",
        "        search_results = await asyncio.gather(*[perform_search_async(query) for query in all_search_queries])\n",
        "\n",
        "        unique_links = set()\n",
        "        for links in search_results:\n",
        "            unique_links.update(links)\n",
        "\n",
        "        # Giới hạn số link được xử lý để tránh vượt quota\n",
        "        limited_links = list(unique_links)[:MAX_LINKS_PER_ITERATION]\n",
        "        link_results = await asyncio.gather(*[process_link(link, user_query, all_search_queries[0]) for link in limited_links])\n",
        "\n",
        "        aggregated_contexts.extend(filter(None, link_results))\n",
        "\n",
        "    print(\"\\nGenerating final report...\")\n",
        "    final_report = await generate_final_report_async(user_query, aggregated_contexts)\n",
        "    print(\"\\n==== FINAL REPORT ====\\n\", final_report)\n",
        "\n",
        "def main():\n",
        "    asyncio.run(async_main())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "oAGqqYgNhkBN",
        "outputId": "31767dbb-56f2-419c-c0e5-a84da3a8b594",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your research query/topic: deepseek R1\n",
            "\n",
            "=== Iteration 1 ===\n",
            "🔍 Fetching and extracting content from: https://www.mining.com/graph-mining-vs-ai-vs-deepseek/\n",
            "🔍 Fetching and extracting content from: https://www.linkedin.com/pulse/deepseek-vs-chatgpt-redefining-ais-battlefront-jason-ledbetter-dtxve/\n",
            "🔍 Fetching and extracting content from: https://www.techtarget.com/WhatIs/feature/DeepSeek-explained-Everything-you-need-to-know\n",
            "🔍 Fetching and extracting content from: https://www.reuters.com/technology/artificial-intelligence/what-is-deepseek-why-is-it-disrupting-ai-sector-2025-01-27/\n",
            "🔍 Fetching and extracting content from: https://www.getguru.com/reference/deepseek\n",
            "🔍 Fetching and extracting content from: https://www.analyticsvidhya.com/blog/2025/01/ai-application-with-deepseek-v3/\n",
            "🔍 Fetching and extracting content from: https://tldv.io/blog/what-is-deepseek/\n",
            "🔍 Fetching and extracting content from: https://www.geeky-gadgets.com/best-ai-for-data-science/\n",
            "🔍 Fetching and extracting content from: https://ai.nd.edu/news/deepseek-explained-what-is-it-and-is-it-safe-to-use/\n",
            "🔍 Fetching and extracting content from: https://www.prompthub.us/blog/deepseek-r-1-model-overview-and-how-it-ranks-against-openais-o1\n",
            "⚠️ No extraction result for URL: https://www.mining.com/graph-mining-vs-ai-vs-deepseek/\n",
            "⚠️ No extraction result for URL: https://www.linkedin.com/pulse/deepseek-vs-chatgpt-redefining-ais-battlefront-jason-ledbetter-dtxve/\n",
            "⚠️ No extraction result for URL: https://www.reuters.com/technology/artificial-intelligence/what-is-deepseek-why-is-it-disrupting-ai-sector-2025-01-27/\n",
            "❌ Groq API Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01hr78ts1afe48z8r673sb3fy6` service tier `on_demand` on tokens per minute (TPM): Limit 5000, Used 4651, Requested 906. Please try again in 6.675s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "❌ Groq API Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01hr78ts1afe48z8r673sb3fy6` service tier `on_demand` on tokens per minute (TPM): Limit 5000, Used 4637, Requested 882. Please try again in 6.225s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "❌ Groq API Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01hr78ts1afe48z8r673sb3fy6` service tier `on_demand` on tokens per minute (TPM): Limit 5000, Used 4248, Requested 995. Please try again in 2.91s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "\n",
            "=== Iteration 2 ===\n",
            "🔍 Fetching and extracting content from: https://www.mining.com/graph-mining-vs-ai-vs-deepseek/\n",
            "🔍 Fetching and extracting content from: https://www.linkedin.com/pulse/deepseek-vs-chatgpt-redefining-ais-battlefront-jason-ledbetter-dtxve/\n",
            "🔍 Fetching and extracting content from: https://www.techtarget.com/WhatIs/feature/DeepSeek-explained-Everything-you-need-to-know\n",
            "🔍 Fetching and extracting content from: https://www.reuters.com/technology/artificial-intelligence/what-is-deepseek-why-is-it-disrupting-ai-sector-2025-01-27/\n",
            "🔍 Fetching and extracting content from: https://www.getguru.com/reference/deepseek\n",
            "🔍 Fetching and extracting content from: https://www.analyticsvidhya.com/blog/2025/01/ai-application-with-deepseek-v3/\n",
            "🔍 Fetching and extracting content from: https://tldv.io/blog/what-is-deepseek/\n",
            "🔍 Fetching and extracting content from: https://www.geeky-gadgets.com/best-ai-for-data-science/\n",
            "🔍 Fetching and extracting content from: https://ai.nd.edu/news/deepseek-explained-what-is-it-and-is-it-safe-to-use/\n",
            "🔍 Fetching and extracting content from: https://www.prompthub.us/blog/deepseek-r-1-model-overview-and-how-it-ranks-against-openais-o1\n",
            "⚠️ No extraction result for URL: https://www.mining.com/graph-mining-vs-ai-vs-deepseek/\n",
            "⚠️ No extraction result for URL: https://www.reuters.com/technology/artificial-intelligence/what-is-deepseek-why-is-it-disrupting-ai-sector-2025-01-27/\n",
            "⚠️ No extraction result for URL: https://www.linkedin.com/pulse/deepseek-vs-chatgpt-redefining-ais-battlefront-jason-ledbetter-dtxve/\n",
            "❌ Groq API Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01hr78ts1afe48z8r673sb3fy6` service tier `on_demand` on tokens per minute (TPM): Limit 5000, Used 5153, Requested 1041. Please try again in 14.328s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "❌ Groq API Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01hr78ts1afe48z8r673sb3fy6` service tier `on_demand` on tokens per minute (TPM): Limit 5000, Used 5148, Requested 989. Please try again in 13.654s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "❌ Groq API Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01hr78ts1afe48z8r673sb3fy6` service tier `on_demand` on tokens per minute (TPM): Limit 5000, Used 5006, Requested 1171. Please try again in 14.128s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "❌ Groq API Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01hr78ts1afe48z8r673sb3fy6` service tier `on_demand` on tokens per minute (TPM): Limit 5000, Used 4518, Requested 1161. Please try again in 8.148s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
            "\n",
            "Generating final report...\n",
            "\n",
            "==== FINAL REPORT ====\n",
            " No relevant content found for: deepseek R1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dùng Together AI và DeepSeek-R1-Distill-Llama-70B"
      ],
      "metadata": {
        "id": "WmU1O3cYuqG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install together"
      ],
      "metadata": {
        "id": "Wvft_4ThvBVQ",
        "outputId": "5c179d17-07b7-4a8d-c75b-4d0c1757f61b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting together\n",
            "  Downloading together-1.4.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.3 in /usr/local/lib/python3.11/dist-packages (from together) (3.11.11)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from together) (8.1.8)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from together) (0.2.2)\n",
            "Requirement already satisfied: filelock<4.0.0,>=3.13.1 in /usr/local/lib/python3.11/dist-packages (from together) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from together) (1.26.4)\n",
            "Requirement already satisfied: pillow<12.0.0,>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from together) (11.1.0)\n",
            "Requirement already satisfied: pyarrow>=10.0.1 in /usr/local/lib/python3.11/dist-packages (from together) (17.0.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.6.3 in /usr/local/lib/python3.11/dist-packages (from together) (2.10.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from together) (2.32.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.8.1 in /usr/local/lib/python3.11/dist-packages (from together) (13.9.4)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from together) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.2 in /usr/local/lib/python3.11/dist-packages (from together) (4.67.1)\n",
            "Requirement already satisfied: typer<0.16,>=0.9 in /usr/local/lib/python3.11/dist-packages (from together) (0.15.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.18.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.6.3->together) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.6.3->together) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.6.3->together) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->together) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->together) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->together) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->together) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.8.1->together) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.8.1->together) (2.18.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.16,>=0.9->together) (1.5.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.8.1->together) (0.1.2)\n",
            "Downloading together-1.4.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.8/73.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: together\n",
            "Successfully installed together-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import asyncio\n",
        "import aiohttp\n",
        "from tavily import TavilyClient\n",
        "from together import Together\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# =======================\n",
        "# Configuration Constants\n",
        "# =======================\n",
        "TOGETHER_API_KEY = userdata.get(\"TOGETHER_API_KEY\")\n",
        "TAVILY_API_KEY = userdata.get(\"TAVILY_API_KEY\")\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")  # OpenAI API Key\n",
        "\n",
        "DEFAULT_MODEL_TOGETHER = \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\"\n",
        "FINAL_REPORT_MODEL_OPENAI = \"o1-mini\"  # OpenAI model hỗ trợ output dài\n",
        "\n",
        "MAX_LINKS_PER_ITERATION = 10\n",
        "MAX_TOKENS_INPUT = 4000\n",
        "\n",
        "# Khởi tạo API client\n",
        "together_client = Together(api_key=TOGETHER_API_KEY)\n",
        "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)\n",
        "\n",
        "# ============================\n",
        "# Asynchronous Helper Functions\n",
        "# ============================\n",
        "\n",
        "async def call_together_ai_async(messages, model=DEFAULT_MODEL_TOGETHER):\n",
        "    \"\"\"Gửi yêu cầu đến Together AI.\"\"\"\n",
        "    try:\n",
        "        response = await asyncio.to_thread(\n",
        "            together_client.chat.completions.create,\n",
        "            messages=messages,\n",
        "            model=model,\n",
        "            temperature=0.7,\n",
        "            max_tokens=500\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Together AI Error: {e}\")\n",
        "        return None\n",
        "\n",
        "async def call_openai_async(messages, model=FINAL_REPORT_MODEL_OPENAI):\n",
        "    \"\"\"Gửi yêu cầu đến OpenAI API để tạo báo cáo dài.\"\"\"\n",
        "    try:\n",
        "        response = await asyncio.to_thread(\n",
        "            openai_client.chat.completions.create,\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            # temperature=0.7, # o1 do not support this param\n",
        "            max_completion_tokens=4000  # Hỗ trợ output dài\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ OpenAI Error: {e}\")\n",
        "        return None\n",
        "\n",
        "async def generate_search_queries_async(session, user_query):\n",
        "    \"\"\"Tạo danh sách truy vấn tìm kiếm dựa trên chủ đề của người dùng.\"\"\"\n",
        "    prompt = (\n",
        "        \"You are an expert research assistant. Given the user's query, generate up to four distinct, \"\n",
        "        \"precise search queries that would help gather comprehensive information on the topic. \"\n",
        "        \"Return only a Python list of strings, for example: ['query1', 'query2', 'query3'].\"\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful and precise research assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "\n",
        "    response = await call_together_ai_async(messages)\n",
        "\n",
        "    if response:\n",
        "        response_cleaned = re.sub(r\"<think>.*?</think>\", \"\", response, flags=re.DOTALL).strip()\n",
        "        try:\n",
        "            if response_cleaned.startswith(\"[\") and response_cleaned.endswith(\"]\"):\n",
        "                search_queries = eval(response_cleaned)\n",
        "                if isinstance(search_queries, list) and all(isinstance(q, str) for q in search_queries):\n",
        "                    return search_queries\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Error parsing search queries: {e}\\nResponse: {response_cleaned}\")\n",
        "\n",
        "    print(\"⚠️ Using default search queries due to parsing error.\")\n",
        "    return [\n",
        "        f\"{user_query} overview\",\n",
        "        f\"{user_query} latest developments\",\n",
        "        f\"{user_query} applications\",\n",
        "        f\"{user_query} comparisons\"\n",
        "    ]\n",
        "\n",
        "async def perform_search_async(session, query):\n",
        "    \"\"\"Thực hiện tìm kiếm trên Tavily và trả về danh sách URL hợp lệ.\"\"\"\n",
        "    try:\n",
        "        response = await asyncio.to_thread(\n",
        "            tavily_client.search,\n",
        "            query=query,\n",
        "            max_results=10,\n",
        "            include_raw_content=True,\n",
        "            search_depth=\"advanced\"\n",
        "        )\n",
        "        links = [item.get(\"url\") for item in response.get(\"results\", []) if \"url\" in item]\n",
        "        valid_links = [url for url in links if \"youtube.com\" not in url and \"deepseek.com\" not in url]\n",
        "        return valid_links\n",
        "    except Exception as e:\n",
        "        print(\"❌ Error performing Tavily search:\", e)\n",
        "        return []\n",
        "\n",
        "async def extract_webpage_text_async(url):\n",
        "    \"\"\"Trích xuất nội dung từ một trang web.\"\"\"\n",
        "    try:\n",
        "        response = await asyncio.to_thread(tavily_client.extract, urls=[url], include_images=False)\n",
        "        if \"results\" in response and response[\"results\"]:\n",
        "            return response[\"results\"][0].get(\"raw_content\", \"\")[:MAX_TOKENS_INPUT]\n",
        "        print(f\"⚠️ No extraction result for URL: {url}\")\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error extracting webpage text with Tavily for {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "async def process_link(session, link, user_query, search_query):\n",
        "    \"\"\"Xử lý từng link: trích xuất nội dung.\"\"\"\n",
        "    print(f\"🔍 Fetching and extracting content from: {link}\")\n",
        "    page_text = await extract_webpage_text_async(link)\n",
        "    return page_text if page_text else None\n",
        "\n",
        "async def generate_final_report_async(session, user_query, all_contexts):\n",
        "    \"\"\"Tạo báo cáo cuối cùng dựa trên nội dung trích xuất, sử dụng OpenAI API để có độ dài cao hơn.\"\"\"\n",
        "    if not all_contexts:\n",
        "        return f\"⚠️ Limited information found for: {user_query}.\"\n",
        "\n",
        "    context_combined = \"\\n\".join(all_contexts)\n",
        "    prompt = (\n",
        "        \"You are an AI research assistant. Based on the gathered contexts and original query, \"\n",
        "        \"write a **detailed and structured** long-form report. \"\n",
        "        \"Ensure that the content is well-organized and provides deep insights.\"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"assistant\", \"content\": \"You are a skilled long-form report writer.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"User Query: {user_query}\\n\\nExtracted Content:\\n{context_combined}\\n\\n{prompt}\"}\n",
        "    ]\n",
        "    report = await call_openai_async(messages)  # ✅ Chuyển qua OpenAI model\n",
        "    return report if report else \"⚠️ No significant data available.\"\n",
        "\n",
        "# =========================\n",
        "# Main Asynchronous Routine\n",
        "# =========================\n",
        "async def async_main():\n",
        "    user_query = input(\"Enter your research query/topic: \").strip()\n",
        "    iteration_limit = 2\n",
        "\n",
        "    aggregated_contexts = []\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        all_search_queries = await generate_search_queries_async(session, user_query)\n",
        "        if not all_search_queries:\n",
        "            print(\"❌ No search queries generated. Exiting.\")\n",
        "            return\n",
        "\n",
        "        for iteration in range(iteration_limit):\n",
        "            print(f\"\\n=== Iteration {iteration + 1} ===\")\n",
        "            search_results = await asyncio.gather(*[perform_search_async(session, query) for query in all_search_queries])\n",
        "\n",
        "            unique_links = set()\n",
        "            for links in search_results:\n",
        "                unique_links.update(links)\n",
        "\n",
        "            limited_links = list(unique_links)[:MAX_LINKS_PER_ITERATION]\n",
        "            link_results = await asyncio.gather(*[process_link(session, link, user_query, all_search_queries[0]) for link in limited_links])\n",
        "\n",
        "            aggregated_contexts.extend(filter(None, link_results))\n",
        "\n",
        "        print(\"\\nGenerating final report...\")\n",
        "        final_report = await generate_final_report_async(session, user_query, aggregated_contexts)\n",
        "        print(\"\\n==== FINAL REPORT ====\\n\", final_report)\n",
        "\n",
        "def main():\n",
        "    asyncio.run(async_main())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "D3mMfExvus-S",
        "outputId": "e0632109-ea38-46ac-c6e4-578e70921f59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your research query/topic: deepseek R1\n",
            "\n",
            "=== Iteration 1 ===\n",
            "🔍 Fetching and extracting content from: https://mashable.com/article/deepseek-ai-hands-on\n",
            "🔍 Fetching and extracting content from: https://console.groq.com/docs/model/deepseek-r1-distill-llama-70b\n",
            "🔍 Fetching and extracting content from: https://www.technologyreview.com/2025/01/24/1110526/china-deepseek-top-ai-despite-sanctions/\n",
            "🔍 Fetching and extracting content from: https://www.geeky-gadgets.com/hardware-requirements-for-deepseek-r1-ai-models/\n",
            "🔍 Fetching and extracting content from: https://www.geeky-gadgets.com/deepseek-r1-review/\n",
            "🔍 Fetching and extracting content from: https://www.giskard.ai/knowledge/deepseek-r1-complete-analysis-of-performance-and-limitations\n",
            "🔍 Fetching and extracting content from: https://arxiv.org/abs/2501.12948\n",
            "🔍 Fetching and extracting content from: https://apxml.com/posts/gpu-requirements-deepseek-r1\n",
            "🔍 Fetching and extracting content from: https://azure.microsoft.com/en-us/blog/deepseek-r1-is-now-available-on-azure-ai-foundry-and-github/\n",
            "🔍 Fetching and extracting content from: https://textcortex.com/post/deepseek-r1-review\n",
            "\n",
            "=== Iteration 2 ===\n",
            "🔍 Fetching and extracting content from: https://mashable.com/article/deepseek-ai-hands-on\n",
            "🔍 Fetching and extracting content from: https://console.groq.com/docs/model/deepseek-r1-distill-llama-70b\n",
            "🔍 Fetching and extracting content from: https://www.technologyreview.com/2025/01/24/1110526/china-deepseek-top-ai-despite-sanctions/\n",
            "🔍 Fetching and extracting content from: https://www.geeky-gadgets.com/hardware-requirements-for-deepseek-r1-ai-models/\n",
            "🔍 Fetching and extracting content from: https://www.geeky-gadgets.com/deepseek-r1-review/\n",
            "🔍 Fetching and extracting content from: https://www.giskard.ai/knowledge/deepseek-r1-complete-analysis-of-performance-and-limitations\n",
            "🔍 Fetching and extracting content from: https://arxiv.org/abs/2501.12948\n",
            "🔍 Fetching and extracting content from: https://apxml.com/posts/gpu-requirements-deepseek-r1\n",
            "🔍 Fetching and extracting content from: https://azure.microsoft.com/en-us/blog/deepseek-r1-is-now-available-on-azure-ai-foundry-and-github/\n",
            "🔍 Fetching and extracting content from: https://textcortex.com/post/deepseek-r1-review\n",
            "\n",
            "Generating final report...\n",
            "\n",
            "==== FINAL REPORT ====\n",
            " # DeepSeek R1: A Comprehensive Analysis of China's Emerging AI Powerhouse\n",
            "\n",
            "## Table of Contents\n",
            "\n",
            "1. [Introduction](#introduction)\n",
            "2. [Background](#background)\n",
            "   - [DeepSeek AI: The Rise of a Chinese AI Startup](#deepseek-ai-the-rise-of-a-chinese-ai-startup)\n",
            "   - [Launch of DeepSeek R1](#launch-of-deepseek-r1)\n",
            "3. [Technical Specifications](#technical-specifications)\n",
            "   - [Model Architecture](#model-architecture)\n",
            "   - [Training Methodology](#training-methodology)\n",
            "   - [Performance Metrics](#performance-metrics)\n",
            "4. [Comparative Analysis](#comparative-analysis)\n",
            "   - [Against OpenAI's Offerings](#against-openais-offerings)\n",
            "   - [Benchmark Performance](#benchmark-performance)\n",
            "   - [Strengths and Weaknesses](#strengths-and-weaknesses)\n",
            "5. [Accessibility and Cost](#accessibility-and-cost)\n",
            "   - [Open-Source Nature](#open-source-nature)\n",
            "   - [Pricing Structure](#pricing-structure)\n",
            "6. [Practical Applications and Use Cases](#practical-applications-and-use-cases)\n",
            "7. [Hardware Requirements](#hardware-requirements)\n",
            "   - [Scalable Deployment](#scalable-deployment)\n",
            "   - [GPU Specifications](#gpu-specifications)\n",
            "8. [Challenges and Limitations](#challenges-and-limitations)\n",
            "   - [Hallucinations and Misinformation](#hallucinations-and-misinformation)\n",
            "   - [Handling of Sensitive Topics](#handling-of-sensitive-topics)\n",
            "9. [Future Prospects](#future-prospects)\n",
            "10. [Conclusion](#conclusion)\n",
            "\n",
            "---\n",
            "\n",
            "## Introduction <a name=\"introduction\"></a>\n",
            "\n",
            "In the rapidly evolving landscape of artificial intelligence (AI), large language models (LLMs) have become pivotal in driving advancements across various sectors. Among the notable players emerging globally, China's DeepSeek AI has positioned itself as a formidable contender with its latest release, DeepSeek R1. This report delves into the intricacies of DeepSeek R1, evaluating its technical prowess, competitive stance against industry giants like OpenAI, accessibility, and practical applications.\n",
            "\n",
            "## Background <a name=\"background\"></a>\n",
            "\n",
            "### DeepSeek AI: The Rise of a Chinese AI Startup <a name=\"deepseek-ai-the-rise-of-a-chinese-ai-startup\"></a>\n",
            "\n",
            "Founded in July 2023 by Liang Wenfeng, an alumnus of Zhejiang University with a background in information and electronic engineering, DeepSeek AI has rapidly ascended the ranks of the global AI community. Based in Hangzhou, China, the company was incubated by High-Flyer, a hedge fund established by Liang in 2015. Drawing parallels to OpenAI's Sam Altman, Liang's vision for DeepSeek AI is centered around developing Artificial General Intelligence (AGI) capable of matching or surpassing human capabilities across a spectrum of tasks.\n",
            "\n",
            "### Launch of DeepSeek R1 <a name=\"launch-of-deepseek-r1\"></a>\n",
            "\n",
            "On January 22, 2025, DeepSeek AI unveiled DeepSeek R1, an open-source large language model (LLM) designed to compete directly with leading models like OpenAI's GPT-4 and Claude 3.5. Positioning itself as not only a technically superior alternative but also a more accessible and cost-effective solution, DeepSeek R1 has garnered significant attention within the tech community.\n",
            "\n",
            "## Technical Specifications <a name=\"technical-specifications\"></a>\n",
            "\n",
            "### Model Architecture <a name=\"model-architecture\"></a>\n",
            "\n",
            "DeepSeek R1 is built upon the Llama-3.3-70B-Instruct framework, comprising 70 billion parameters. The model employs the Mixture of Experts (MoE) approach, enhancing its efficiency by selectively activating subsets of parameters tailored to specific tasks. This architecture allows DeepSeek R1 to maintain robust reasoning capabilities while optimizing computational resources.\n",
            "\n",
            "**Variants:**\n",
            "- **DeepSeek-R1-Zero:** A precursor to R1, trained using large-scale reinforcement learning (RL) without supervised fine-tuning (SFT). While demonstrating strong reasoning abilities, it faced challenges such as poor readability and language mixing.\n",
            "- **DeepSeek-R1:** An evolution of R1-Zero, incorporating multi-stage training and cold-start data before reinforcement learning. This refinement addressed the initial limitations, enhancing overall performance and language coherence.\n",
            "\n",
            "### Training Methodology <a name=\"training-methodology\"></a>\n",
            "\n",
            "DeepSeek R1's training regimen showcases innovative methodologies aimed at maximizing efficiency and performance:\n",
            "- **Reinforcement Learning (RL):** Utilized to cultivate reasoning capabilities, enabling the model to refine its responses through iterative feedback mechanisms.\n",
            "- **Supervised Fine-Tuning (SFT):** Introduced in DeepSeek R1 to improve readability and reduce language mixing issues inherent in R1-Zero.\n",
            "- **Knowledge Distillation:** Employed to create smaller, more efficient versions of DeepSeek R1, ensuring scalability and broader accessibility.\n",
            "\n",
            "### Performance Metrics <a name=\"performance-metrics\"></a>\n",
            "\n",
            "DeepSeek R1 has been benchmarked across multiple domains, demonstrating competitive or superior performance compared to established models:\n",
            "- **AIME 2024:** Pass@1 score of 70.0.\n",
            "- **MATH-500:** Pass@1 score of 94.5.\n",
            "- **CodeForces Rating:** Achieved a rating of 1,633.\n",
            "- **AER Polyglot Benchmark:** Scored 57%, placing it just behind proprietary models like OpenAI's O1.\n",
            "\n",
            "## Comparative Analysis <a name=\"comparative-analysis\"></a>\n",
            "\n",
            "### Against OpenAI's Offerings <a name=\"against-openais-offerings\"></a>\n",
            "\n",
            "DeepSeek R1 positions itself as a direct competitor to OpenAI's GPT-4 and its variants. Key differentiators include:\n",
            "- **Cost Efficiency:** At $0.14 per million input tokens, DeepSeek R1 is significantly cheaper than OpenAI's $7.5 for the most powerful reasoning model, o1.\n",
            "- **Open-Source Accessibility:** Licensed under the MIT license, DeepSeek R1 offers transparency and flexibility for developers and researchers.\n",
            "- **Performance:** Claims to match or surpass OpenAI's models in benchmarks related to mathematics, coding, and reasoning tasks.\n",
            "\n",
            "### Benchmark Performance <a name=\"benchmark-performance\"></a>\n",
            "\n",
            "DeepSeek R1 has been evaluated against industry-leading models, showcasing its capabilities:\n",
            "- **Historical Accuracy:** In specialized queries, such as identifying Léon Gambetta as the French politician elected deputy of Marseille in 1869, DeepSeek R1 demonstrated higher accuracy compared to OpenAI's GPT-4o and O1, which provided incorrect answers.\n",
            "- **Reasoning and Problem-Solving:** Utilizes a \"chain of thought\" approach, enabling step-by-step problem-solving akin to ChatGPT o1.\n",
            "\n",
            "### Strengths and Weaknesses <a name=\"strengths-and-weaknesses\"></a>\n",
            "\n",
            "**Strengths:**\n",
            "- **Cost-Effectiveness:** Substantially lower pricing makes it accessible to a broader user base.\n",
            "- **Open-Source Nature:** Facilitates community-driven improvements and transparency.\n",
            "- **Specialized Performance:** Excels in mathematical, coding, and reasoning tasks.\n",
            "\n",
            "**Weaknesses:**\n",
            "- **Hallucinations:** Prone to generating incorrect information, especially on topics beyond its training data.\n",
            "- **Handling of Temporal Data:** Occasionally fails to acknowledge knowledge cutoffs, producing confident but inaccurate responses regarding recent events.\n",
            "- **Resource Intensive for Larger Models:** Requires substantial hardware resources, limiting accessibility for some users.\n",
            "\n",
            "## Accessibility and Cost <a name=\"accessibility-and-cost\"></a>\n",
            "\n",
            "### Open-Source Nature <a name=\"open-source-nature\"></a>\n",
            "\n",
            "DeepSeek R1's open-source framework under the MIT license allows:\n",
            "- **Community Scrutiny:** Experts can inspect and verify the model, enhancing trust in its privacy and security measures.\n",
            "- **Customization:** Developers can tailor the model to specific use cases without restrictions imposed by proprietary licenses.\n",
            "\n",
            "### Pricing Structure <a name=\"pricing-structure\"></a>\n",
            "\n",
            "DeepSeek R1 offers a highly competitive pricing model:\n",
            "- **Web App:** Free to use, facilitating easy access for casual users and developers.\n",
            "- **API Access:** Priced at $0.14 per million input tokens, making it a cost-effective alternative to OpenAI's offerings, which charge $7.5 for similar capacities.\n",
            "\n",
            "**Distilled Models Pricing:**\n",
            "- Smaller variants, such as DeepSeek-R1-Distill-Qwen-1.5B, are even more economical, catering to users with limited computational resources.\n",
            "\n",
            "## Practical Applications and Use Cases <a name=\"practical-applications-and-use-cases\"></a>\n",
            "\n",
            "DeepSeek R1's versatility makes it suitable for a wide range of applications:\n",
            "- **Educational Tools:** Leveraging its mathematical problem-solving capabilities for tutoring and research.\n",
            "- **Software Development:** Assisting in code generation, debugging, and optimization.\n",
            "- **Data Analysis:** Utilizing logical reasoning for strategic planning and data interpretation.\n",
            "- **Content Creation:** Facilitating multilingual processing and summarization tasks for global applications.\n",
            "- **Chatbot Development:** Powering AI-driven conversational agents across various industries.\n",
            "\n",
            "**Case Study:**\n",
            "Stan Schroeder of Mashable tested DeepSeek R1 by requesting the development of a complex web application. DeepSeek R1 successfully generated and refined HTML code, demonstrating its practical utility in real-world scenarios.\n",
            "\n",
            "## Hardware Requirements <a name=\"hardware-requirements\"></a>\n",
            "\n",
            "### Scalable Deployment <a name=\"scalable-deployment\"></a>\n",
            "\n",
            "DeepSeek R1 is designed for scalability, offering models ranging from 1.5 billion parameters to a staggering 671 billion parameters. This scalability ensures that users can select a model that aligns with their specific needs and available resources.\n",
            "\n",
            "### GPU Specifications <a name=\"gpu-specifications\"></a>\n",
            "\n",
            "**General Requirements:**\n",
            "- **Smaller Models (1.5B):** \n",
            "  - **CPU:** No older than 10 years.\n",
            "  - **RAM:** At least 8 GB.\n",
            "  - **GPU:** Not required.\n",
            "- **Mid-Range Models (7B-8B):**\n",
            "  - **GPU:** At least 8 GB of VRAM for enhanced performance.\n",
            "- **Large Models (70B-671B):**\n",
            "  - **GPU:** High-end setups, such as NVIDIA A100 80GB with multiple GPU configurations (e.g., x16 for 671B parameters).\n",
            "\n",
            "**Quantization:**\n",
            "- **4-Bit Quantization:** Reduces VRAM requirements, making larger models more accessible. For example, the 70B model requires approximately 40 GB of VRAM under 4-bit quantization, achievable with multi-GPU setups like two NVIDIA RTX 4090 24GB GPUs.\n",
            "\n",
            "**Deployment Tips:**\n",
            "- **Distributed GPU Setups:** Essential for handling the VRAM demands of larger models.\n",
            "- **Optimization Techniques:** Adjusting batch sizes and processing settings can facilitate running DeepSeek R1 on lower-spec GPUs, albeit with reduced performance.\n",
            "\n",
            "## Challenges and Limitations <a name=\"challenges-and-limitations\"></a>\n",
            "\n",
            "### Hallucinations and Misinformation <a name=\"hallucinations-and-misinformation\"></a>\n",
            "\n",
            "Despite its strengths, DeepSeek R1 exhibits a notable tendency to generate hallucinations—confident yet incorrect responses. This issue is particularly pronounced when the model is queried on topics beyond its training data.\n",
            "\n",
            "**Examples:**\n",
            "- **Historical Queries:** Incorrectly identified Georges Clemenceau and Adolphe Thiers as the French politician elected deputy of Marseille in 1869, when the accurate answer is Léon Gambetta.\n",
            "- **Temporal Queries:** Provided fabricated details about the 2024 Golden Globe Awards, despite a training data cutoff in December 2023.\n",
            "\n",
            "### Handling of Sensitive Topics <a name=\"handling-of-sensitive-topics\"></a>\n",
            "\n",
            "DeepSeek R1's ability to manage politically sensitive or complex topics is mixed. While it demonstrates competency in structured reasoning tasks, its susceptibility to misinformation can pose challenges in applications requiring high factual accuracy and reliability.\n",
            "\n",
            "## Future Prospects <a name=\"future-prospects\"></a>\n",
            "\n",
            "DeepSeek AI aims to continue its trajectory of innovation and expansion:\n",
            "- **Enhanced Training Techniques:** Further refining training methodologies to mitigate hallucinations and improve temporal awareness.\n",
            "- **Community Engagement:** Leveraging its open-source status to foster a robust community for continuous improvement and diversification of use cases.\n",
            "- **Strategic Collaborations:** Partnering with platforms like Microsoft Azure to increase accessibility and integration capabilities.\n",
            "- **Hardware Advancements:** Developing more efficient model architectures to reduce hardware dependency, making DeepSeek R1 more accessible to a broader audience.\n",
            "\n",
            "## Conclusion <a name=\"conclusion\"></a>\n",
            "\n",
            "DeepSeek R1 represents a significant milestone in the global AI landscape, particularly within the Chinese tech ecosystem. By offering a highly capable, open-source, and cost-effective alternative to established models like OpenAI's GPT-4, DeepSeek AI is democratizing access to advanced AI technologies. While challenges such as hallucinations and hardware requirements persist, the continuous advancements and community-driven improvements position DeepSeek R1 as a formidable player poised to influence the future of AI-driven applications.\n",
            "\n",
            "As the AI industry navigates the balance between performance, accessibility, and ethical considerations, models like DeepSeek R1 underscore the importance of innovation driven by diverse global perspectives and needs. The coming years will be pivotal in observing how DeepSeek AI evolves and shapes the trajectory of large language models worldwide.\n"
          ]
        }
      ]
    }
  ]
}